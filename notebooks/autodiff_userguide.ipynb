{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation on JAX Automatic Differentiation\n",
    "Here describes the automatic differetiation notation used throughout jax-gcm and how to set up your own jax gradient code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation \n",
    "\n",
    "To take gradients of functions in jax-gcm we will be using the jax function jax.vjp().  While jax.grad() can also take derivatives, it is a special case of jax.vjp() and requires the output of the function being differentiated to be a scaler.  jax.vjp() is more generalized and can be used in the cases we require (a.k.a. functions that have outputs other than scalers). \n",
    "\n",
    "jax.vjp() takes in the function being differetiated and the function parameters and outputs a set of primals (which we won't worry about too much) and the function that will output the gradients. Here is an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivate of my function with respect to x is: [4. 4. 4.]\n",
      "The derivate of my function with respect to y is: [ 0.  3. 12.]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Example of the funtion to be differentiated \n",
    "def my_func(x, y):  # Input two parameters (can be vectors of the same shape) \n",
    "    return x**2 + y**3  # Outputs one vector (or scaler )\n",
    "\n",
    "# Define a function to calculate the gradient\n",
    "def grad_my_func(f, x, y): # Parameters should be the same as my_func\n",
    "    primals, f_vjp = jax.vjp(f, x, y)  # Creates primals and \"derivative function\"\n",
    "    input = (jnp.ones_like(primals)) # Input into f_vjp.  Must be the shape of the output of my_func \n",
    "                                # I think it should be initialized with all ones to get the gradient value with\n",
    "                                # respect to the parameters of my_func \n",
    "    df_dx, df_dy = f_vjp(input) #Takes derivate with respect to each parameter\n",
    "    return df_dx, df_dy\n",
    "\n",
    "# Test \n",
    "xx = 2*jnp.ones(3) # x input \n",
    "yy = jnp.arange(0.0, 3.0, 1) # y input\n",
    "df_dx, df_dy = grad_my_func(my_func, xx, yy) # Call gradient function\n",
    "print('The derivate of my function with respect to x is:', df_dx)\n",
    "print('The derivate of my function with respect to y is:',df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best practices when creating gradient functions\n",
    "- To define the gradient of a function, put 'grad_' at the front of the function name.\n",
    "- When calling jax.vjp(), always call the output 'primals, f_vjp'.\n",
    "- Initialize input values to be inserted back into f_vjp(). (You can use the primals (which are the outputs of the function for the given input) but you need to make sure that you create the same object type with all ones so that the gradient is accurate)\n",
    "- When calculating the partial derivatives, call them d(function output)_d(variable the derivative is with respect to).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other important tips\n",
    "- JAX cannot take the gradient of integer values, so make sure to initialize the function parameter values with floats (or an array of floats). \n",
    "- More to come..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-gcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
