{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation on JAX Automatic Differentiation\n",
    "Here describes the automatic differetiation notation used throughout jax-gcm and how to set up your own jax gradient code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation \n",
    "\n",
    "To take gradients of functions in jax-gcm we will be using the jax function jax.vjp().  While jax.grad() can also take derivatives, it is a special case of jax.vjp() and requires the output of the function being differentiated to be a scaler.  jax.vjp() is more generalized and can be used in the cases we require (a.k.a. functions that have outputs other than scalers). \n",
    "\n",
    "jax.vjp() takes in the function being differetiated and the function parameters and outputs a set of primals (which we won't worry about too much, but are the outputs from the forward model) and the function that will output the gradients. Here is an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivate of my function with respect to x is: [4. 4. 4.]\n",
      "The derivate of my function with respect to y is: [ 0.  3. 12.]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Example of the funtion to be differentiated \n",
    "def my_func(x, y):  # Input two parameters (can be vectors of the same shape) \n",
    "    return x**2 + y**3  # Outputs one vector (or scaler )\n",
    "\n",
    "# Define a function to calculate the gradient\n",
    "def grad_my_func(f, x, y): # Parameters should be the same as my_func\n",
    "    primals, f_vjp = jax.vjp(f, x, y)  # Creates primals and \"derivative function\"\n",
    "    input = (jnp.ones_like(primals)) # Input into f_vjp.  Must be the shape of the output of my_func \n",
    "                                # I think it should be initialized with all ones to get the gradient value with\n",
    "                                # respect to the parameters of my_func \n",
    "    df_dx, df_dy = f_vjp(input) #Takes derivate with respect to each parameter\n",
    "    return df_dx, df_dy\n",
    "\n",
    "# Test \n",
    "xx = 2*jnp.ones(3) # x input \n",
    "yy = jnp.arange(0.0, 3.0, 1) # y input\n",
    "df_dx, df_dy = grad_my_func(my_func, xx, yy) # Call gradient function\n",
    "print('The derivate of my function with respect to x is:', df_dx)\n",
    "print('The derivate of my function with respect to y is:',df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best practices when creating gradient functions\n",
    "- To define the gradient of a function, put 'grad_' at the front of the function name.\n",
    "- When calling jax.vjp(), always call the output 'primals, f_vjp'.\n",
    "- Initialize input values to be inserted back into f_vjp(). (You can use the shape of the primals (which are the outputs of the function for the given input) but you need to make sure that you create the same object type with all ones so that the gradient is accurate)\n",
    "- When calculating the partial derivatives, call them d(function output)_d(variable the derivative is with respect to).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other important tips\n",
    "- JAX cannot take the gradient of integer values, so make sure to initialize the function parameter values with floats (or an array of floats). \n",
    "- If there are multiple output objects, jax.vjp() sums over the gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking gradients using jax.jvp (jacobian vector product)\n",
    "\n",
    "- Sometime it might be better to use jax.jvp instead of vjp\n",
    "- JVP allows for computing the gradient for individual parameters (one at a time)\n",
    "- JVP also computes the primals (the output of the forward function)\n",
    "- The third argument of jax.jvp is called the tangent.  It has the same shape as the parameters of the function \n",
    "and if you set a parameter value = 1.0, it takes the gradient with respect to that parameter\n",
    "- Below are some examples, along with explanations for notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivate of my function with respect to x is: [4. 4. 4.]\n",
      "The derivate of my function with respect to y is: [ 0.  3. 12.]\n",
      "The derivate of my function summed over all partials is: [ 4.  7. 16.]\n"
     ]
    }
   ],
   "source": [
    "# Use JVP to take gradient with respect to x\n",
    "def jac_my_func_x(f, x, y): \n",
    "    primals, df_dx = jax.jvp(f, [x, y], \n",
    "                        [jnp.ones_like(xx), # set ones for parameter you want to take the gradient with respect to\n",
    "                         jnp.zeros_like(yy)]) # set all other parameters to zeros\n",
    "    return df_dx\n",
    "\n",
    "# Use JVP to take gradient with respect to y\n",
    "def jac_my_func_y(f, x, y): \n",
    "    primals, df_dy = jax.jvp(f, [x, y], \n",
    "                        [jnp.zeros_like(xx), \n",
    "                         jnp.ones_like(yy)])\n",
    "    return df_dy\n",
    "\n",
    "# If you set all the tangents with ones: \n",
    "def jac_my_func_all(f, x, y):\n",
    "    primals, df = jax.jvp(f, [x, y], \n",
    "                        [jnp.ones_like(xx), \n",
    "                         jnp.ones_like(yy)])\n",
    "    return df\n",
    "\n",
    "df_dx = jac_my_func_x(my_func, xx, yy)\n",
    "df_dy = jac_my_func_y(my_func, xx, yy)\n",
    "df = jac_my_func_all(my_func, xx, yy)\n",
    "print('The derivate of my function with respect to x is:', df_dx)\n",
    "print('The derivate of my function with respect to y is:', df_dy)\n",
    "print('The derivate of my function summed over all partials is:', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JVP vs VJP for functions with multiple output components\n",
    "- JVP: calculates each function output gradient with respect to the parameter of interest\n",
    "- VJP: sums over the function output gradients for all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivate of my function with respect to x from VJP is: [8. 8. 8.]\n",
      "The derivate of my function with respect to y from VJP is: [ 0.  6. 24.]\n",
      "The derivate of my function with respect to x from JVP is: (Array([4., 4., 4.], dtype=float32), Array([4., 4., 4.], dtype=float32))\n",
      "The derivate of my function with respect to y from JVP is: (Array([ 0.,  3., 12.], dtype=float32), Array([ 0.,  3., 12.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# JVP and VJP have different behavior with multiple function outputs\n",
    "def my_multi_func(x, y): \n",
    "    return x**2 + y**3, x**2 + y**3 \n",
    "\n",
    "# VJP\n",
    "def grad_my_multi_func(f, x, y): \n",
    "    primals, f_vjp = jax.vjp(f, x, y) \n",
    "    input = (jnp.ones_like(primals[0]), jnp.ones_like(primals[1]))\n",
    "    df_dx, df_dy = f_vjp(input) \n",
    "    return df_dx, df_dy\n",
    "\n",
    "# JVP with respect to x\n",
    "def jac_my_multi_func_x(f, x, y):\n",
    "    primals, df_dx = jax.jvp(f, [x, y], \n",
    "                    [jnp.ones_like(xx), \n",
    "                        jnp.zeros_like(yy)])\n",
    "    return df_dx\n",
    "\n",
    "# JVP with respect to y\n",
    "def jac_my_multi_func_y(f, x, y):\n",
    "    primals, df_dy = jax.jvp(f, [x, y], \n",
    "                    [jnp.zeros_like(xx), \n",
    "                        jnp.ones_like(yy)])\n",
    "    return df_dy\n",
    "\n",
    "df_dx_vjp, df_dy_vjp = grad_my_multi_func(my_multi_func, xx, yy) \n",
    "df_dx_jvp = jac_my_multi_func_x(my_multi_func, xx, yy)\n",
    "df_dy_jvp = jac_my_multi_func_y(my_multi_func, xx, yy)\n",
    "print('The derivate of my function with respect to x from VJP is:', df_dx_vjp)\n",
    "print('The derivate of my function with respect to y from VJP is:',df_dy_vjp)\n",
    "print('The derivate of my function with respect to x from JVP is:', df_dx_jvp)\n",
    "print('The derivate of my function with respect to y from JVP is:', df_dy_jvp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivate of my vectorized function with respect to theta from VJP is: (Array([ 4.,  4.,  4.,  0.,  3., 12.], dtype=float32),)\n",
      "The derivate of my vectorized function with respect to theta from JVP is: [4. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# What if there was only one parameter vector inserted into the function? \n",
    "def vectorized_func(theta): # x must be of length 6\n",
    "    return theta[:3]**2 + theta[3:]**3\n",
    "\n",
    "# VJP\n",
    "def grad_v_func(f, theta): \n",
    "    primals, f_vjp = jax.vjp(f, theta) \n",
    "    input = (jnp.ones_like(primals))\n",
    "    df_dtheta = f_vjp(input) \n",
    "    return df_dtheta\n",
    "\n",
    "# JVP with respect to theta (i.e. all the parameters)\n",
    "def jac_v_func_theta(f, theta):\n",
    "    primals, df_dtheta = jax.jvp(f, [theta], [jnp.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0])])\n",
    "    return df_dtheta\n",
    "\n",
    "theta = jnp.array([2.0, 2.0, 2.0, 0.0, 1.0, 2.0])\n",
    "df_dtheta_vjp = grad_v_func(vectorized_func, theta) \n",
    "df_dtheta_jvp = jac_v_func_theta(vectorized_func, theta)\n",
    "print('The derivate of my vectorized function with respect to theta from VJP is:', df_dtheta_vjp)\n",
    "print('The derivate of my vectorized function with respect to theta from JVP is:', df_dtheta_jvp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional thoughts:\n",
    "\n",
    "### VJP: \n",
    "- Sums over each function partial and returns a vector of of the function gradient with respect to each parameter, but summed over each function output partial (i.e. returns a vector of the same shape/size as the function parameters)\n",
    "\n",
    "### JVP: \n",
    "- Sums over each parameter partial and returns a vector of summed partial derivatives for each element of the function output (i.e. returns a vector the same shape/size as the function output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-gcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
